{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06f1439",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "**Python Topics** - deep_learning > activation_functions\n",
    "\n",
    "**Type** - modification\n",
    "\n",
    "**Target Number of Turns (User + Assistant)** - 2+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c09ce",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb234b0",
   "metadata": {},
   "source": [
    "**User**\n",
    "\n",
    "I'm trying to implement a custom activation function in my model. Right now, it's just a placeholder. Can you fill in the function to be a leaky ReLU with an alpha of 0.2? Here's the code: `def custom_activation(x): return x # TODO: Implement leaky ReLU here`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
