{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c82562",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "**Python Topics** - deep_learning > backpropagation_understanding\n",
    "\n",
    "**Type** - modification\n",
    "\n",
    "**Target Number of Turns (User + Assistant)** - 2+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cf6f9",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2ca0d",
   "metadata": {},
   "source": [
    "**User**\n",
    "\n",
    "I'm trying to implement a custom backpropagation step for my neural network, but I'm not sure if I'm updating the biases correctly. Could you take a look and make any necessary corrections? Here's the code: ```python\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Network parameters\n",
    "weights = np.array([[0.5], [0.3]])\n",
    "biases = np.array([0.1])\n",
    "input_data = np.array([1, 2])\n",
    "\n",
    "# Forward pass\n",
    "layer_input = np.dot(input_data, weights) + biases\n",
    "layer_output = sigmoid(layer_input)\n",
    "\n",
    "# Backpropagation\n",
    "error = layer_output - np.array([1])\n",
    "gradient = sigmoid_derivative(layer_output) * error\n",
    "biases_update = np.sum(gradient, axis=0)\n",
    "biases -= biases_update\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
