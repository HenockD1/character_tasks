{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from project_path import PROJECT_PATH\n",
    "sys.path.insert(0, PROJECT_PATH)\n",
    "from src.llm_reviewer.notebook_reviewer import IssueLevel\n",
    "\n",
    "\n",
    "service_account_path = PROJECT_PATH + \"/creds/google__sa.json\"\n",
    "tracking_sheet_id = \"1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4\"\n",
    "delivery_sheet_id = \"1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4\"\n",
    "\n",
    "\n",
    "ISSUE_LEVEL = IssueLevel.MEDIUM\n",
    "DATA_DIR = PROJECT_PATH + '/data/03_01_2024/'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.sheets_utils import download_sheet_as_df\n",
    "\n",
    "\n",
    "contributors_df = download_sheet_as_df(\n",
    "    service_account_path,\n",
    "    tracking_sheet_id,\n",
    "    \"Contributors\"\n",
    ")\n",
    "\n",
    "tasks_sheets_df = pd.concat(\n",
    "    [\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            tracking_sheet_id,\n",
    "            \"Conversations_Batch_2\"\n",
    "        ),\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            tracking_sheet_id,\n",
    "            \"Conversations_Batch_3\"\n",
    "        ),\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            tracking_sheet_id,\n",
    "            \"Conversations_Batch_4\"\n",
    "        ),\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            tracking_sheet_id,\n",
    "            \"Conversations_Batch_5\"\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "tasks_sheets_df[\"completion_date\"] = pd.to_datetime(tasks_sheets_df[\"completion_date\"], format=\"mixed\").dt.date\n",
    "tasks_sheets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = tasks_sheets_df.copy()#df[df[\"completion_date\"] <= pd.to_datetime(\"2023/12/27\").date()]\n",
    "old_completed = old[old[\"completion_status\"] == \"Done\"]\n",
    "old_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivered_df = pd.concat(\n",
    "    [\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            delivery_sheet_id,\n",
    "            \"Batch 1\"\n",
    "        ),\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            delivery_sheet_id,\n",
    "            \"Batch 2\"\n",
    "        ),\n",
    "        download_sheet_as_df(\n",
    "            service_account_path,\n",
    "            delivery_sheet_id,\n",
    "            \"Batch 3\"\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "delivered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_task_link(row):\n",
    "    if isinstance(row[\"task_link\"], float) and pd.isna(row[\"task_link\"]):\n",
    "        row[\"task_link\"] = row[\"#REF!\"]\n",
    "    return row\n",
    "\n",
    "undelivered_old = old_completed[~old_completed[\"task_link\"].isin(delivered_df[\"task_link\"])]\n",
    "try:\n",
    "    undelivered_old = undelivered_old.apply(fix_task_link, axis=1)\n",
    "except KeyError:\n",
    "    pass\n",
    "undelivered_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "import nbformat\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "\n",
    "\n",
    "def download_and_parse_notebook(service_account_file, file_id):\n",
    "    # Authenticate with the service account\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        service_account_file, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # Request to download the file\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "    # Download the file\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download progress: %d%%.\" % int(status.progress() * 100))\n",
    "\n",
    "    # Move the buffer's pointer to the beginning\n",
    "    fh.seek(0)\n",
    "\n",
    "    # Open the notebook\n",
    "    nb_parsed_notebook = nbformat.read(fh, as_version=4)\n",
    "\n",
    "    return {'file_id': file_id, 'nb_parsed_notebook': nb_parsed_notebook}\n",
    "\n",
    "\n",
    "def threading_processor(service_account_path, file_id, results):\n",
    "    results.append(download_and_parse_notebook(service_account_path, file_id))\n",
    "\n",
    "\n",
    "df = undelivered_old\n",
    "\n",
    "threads = []\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_file_id_from_task_link(task_link):\n",
    "    try:\n",
    "        return task_link.split(\"/\")[-1]\n",
    "    except Exception as e:\n",
    "        print('ERROR' + '='*60)\n",
    "        print(task_link)\n",
    "        return None\n",
    "\n",
    "file_ids = df[\"task_link\"].apply(get_file_id_from_task_link).dropna().tolist()\n",
    "parsed_conversations = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(download_and_parse_notebook, service_account_path, file_id) for file_id in file_ids]\n",
    "    for future in futures:\n",
    "        try:\n",
    "            result = future.result()\n",
    "            parsed_conversations.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download and parse notebook: {e}\")\n",
    "\n",
    "\n",
    "with open(DATA_DIR + 'parsed_conversations.pkl', 'wb') as f:\n",
    "    pickle.dump(parsed_conversations, f)\n",
    "\n",
    "with open(DATA_DIR + 'parsed_conversations.pkl', 'rb') as f:\n",
    "    parsed_conversations = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(DATA_DIR + 'parsed_conversations.pkl', 'rb') as f:\n",
    "    parsed_conversations = pickle.load(f)\n",
    "    \n",
    "from src.llm_reviewer.notebook_reviewer import review_notebooks\n",
    "\n",
    "OFFSET = 0\n",
    "notebooks = parsed_conversations\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 30\n",
    "total_notebooks = len(notebooks)\n",
    "reviews_pkl_folder = DATA_DIR + 'raw_reviews_pkls/'\n",
    "os.makedirs(reviews_pkl_folder, exist_ok=True)\n",
    "\n",
    "for i in range(OFFSET, total_notebooks, batch_size):\n",
    "    batch_notebooks = notebooks[i:i+batch_size]\n",
    "    print(f\"Reviewing notebooks {i+1} to {min(i+batch_size, total_notebooks)} out of {total_notebooks}\")\n",
    "    batch_reviews = review_notebooks(batch_notebooks, max_threads_per_notebook=6, max_concurrent_notebooks=16)\n",
    "    batch_file_name = f'review_results{i+1}-{min(i+batch_size, total_notebooks)}.pkl'\n",
    "    batch_file_path = os.path.join(reviews_pkl_folder, batch_file_name)\n",
    "    with open(batch_file_path, 'wb') as f:\n",
    "        pickle.dump(batch_reviews, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_reviewer.notebook_reviewer import notebook_reviews_to_df\n",
    "\n",
    "import glob\n",
    "\n",
    "review_files = glob.glob(DATA_DIR + 'raw_reviews_pkls/*.pkl')\n",
    "reviews = []\n",
    "for file_path in review_files:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reviews.extend(pickle.load(f))\n",
    "\n",
    "seen_paths = set()\n",
    "deduped_reviews = []\n",
    "for review in reversed(reviews):\n",
    "    if review is not None and review['nb_path'] not in seen_paths:\n",
    "        seen_paths.add(review['nb_path'])\n",
    "        deduped_reviews.append(review)\n",
    "reviews = list(reversed(deduped_reviews))\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PROMPT = \"\"\"\n",
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model. You only point out things worth mentioning.\n",
    "\n",
    "Attributes to consider:\n",
    "- Code Correctness\n",
    "- Code Efficiency\n",
    "- Best Practices\n",
    "- Code Readability\n",
    "- Code style Consistency\n",
    "\n",
    "**1. Evaluation Criteria Definitions**\n",
    "- Correctness: The code must be devoid of bugs and errors.\n",
    "- Efficiency: The code must be optimized for maximum performance.\n",
    "- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.\n",
    "- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.\n",
    "- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.\n",
    "\n",
    "**2. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "-----\n",
    "\n",
    "You are provided with the issues found in each turn of an interaction between user and AI LLM Assistant.\n",
    "If no issues reported, it means no issues were found.\n",
    "\n",
    "# START OF JUDGMENT MATERIAL\n",
    "{FEEDBACK}\n",
    "# END OF JUDGMENT MATERIAL\n",
    "\n",
    "\n",
    "# Grading rubric\n",
    "\n",
    "### 5 - Excellent\n",
    "- Well Formatted\n",
    "- Correct\n",
    "- Optimal\n",
    "- Highly readable\n",
    "\n",
    "### 4 - Good\n",
    "- Correct but can be slightly optimized in terms of approach / speed / readability\n",
    "\n",
    "### 3 - Acceptable\n",
    "- The code is correct but can be significantly improved.\n",
    "- The code is not readable.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- The code is incorrect / out of scope / has syntax errors.\n",
    "- Looks like it’s copied from ChatGPT.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or Missing code.\n",
    "\n",
    "\n",
    "Given the feedback above, generate a 1 sentence judgment and a score.\n",
    "Your output should be as JSON in the following format:\n",
    "\n",
    "{{\"judgment\": \"single sentence\", \"score\": \"1 to 5 according to rubrics and provided by turn feedback\"}}\n",
    "\n",
    "Take a deep breath.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LANG_PROMPT = \"\"\"\n",
    "You are a concise expert in analyzing and improving English usage by an AI assistant based on Large Lnaguage Model. You only point out things worth mentioning.\n",
    "\n",
    "\n",
    "Attributes to consider:\n",
    "- English correctness, clarity, precision, and conciseness\n",
    "- Alignment with the assistant's identity and the User's query and persona and User's persona, its background and level of knowledge\n",
    "- Markdown style and formatting\n",
    "\n",
    "You will be provided a single turn in the middle of the conversation between a user and an LLM Assistant. Assume that there might be other turns before or after the exchange you are  provided.\n",
    "\n",
    "\n",
    "**1. Identification of Reply Text for Review**\n",
    "- Target for analysis: *Text* Replies generated by the LLM Assistant.\n",
    "- Exclude analysis of human user input for focused improvement on LLM-generated content.\n",
    "- Exclude LLM **code** content, only review **text** parts. Code is for context only.\n",
    "\n",
    "**2. Evaluation Criteria Definitions**\n",
    "- English Correctness: Grammar, syntax, punctuation, and spelling.\n",
    "- Clarity: The ease with which the intended audience(this particular user) can understand the reply.\n",
    "- Precision: The accuracy and specificity of the information provided.\n",
    "- Conciseness: The brevity of the reply while conveying complete information.\n",
    "- Structure: The organization of information and logical flow within the reply.\n",
    "- Relevance: The pertinence of the reply to the user's input.\n",
    "- Leveraging appropriate markdown syntax tools in order to optimize information presented for easier readability and navigation.\n",
    "\n",
    "**3. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "-----\n",
    "\n",
    "You are provided with the issues found in each turn of an interaction between user and AI LLM Assistant.\n",
    "If no issues reported, it means no issues were found.\n",
    "\n",
    "# START OF JUDGMENT MATERIAL\n",
    "{FEEDBACK}\n",
    "# END OF JUDGMENT MATERIAL\n",
    "\n",
    "\n",
    "# Grading rubric\n",
    "\n",
    "### 5 - Excellent\n",
    "- Authentic & Realistic (User) \n",
    "- Well Formatted markdown for ease of consumption and understanding of the information(Assistant)\n",
    "- Maximum usefulness while being to the point (Assistant)\n",
    "- Free of mistakes (Assistant)\n",
    "- Tailored to the user & situation (Assistant)\n",
    "\n",
    "### 4 - Good\n",
    "- Clear but can be optimized with 1 or 2 minor issues.\n",
    "\n",
    "### 3 - Acceptable\n",
    "- You can still understand what’s being said but things can be phrased much better. \n",
    "- Reasoning/Explanations are missing.\n",
    "- Can have some minor mistakes or 1 major mistake.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- It’s hard to understand what’s being said. \n",
    "- Has many minor language mistakes or more than 1 major mistake. \n",
    "- Looks like it’s copied from ChatGPT.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or Missing responses.\n",
    "\n",
    "\n",
    "Given the feedback above, generate a 1 sentence judgment and a score.\n",
    "Your output should be as JSON in the following format:\n",
    "\n",
    "{{\"judgment\": \"single sentence\", \"score\": \"1 to 5 according to rubrics and provided by turn feedback\"}}\n",
    "\n",
    "Take a deep breath.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from src.llm_reviewer.llm_api import make_llm_request, LLMAPIFactory\n",
    "from src.llm_reviewer.constants import PATH_TO_SECRETS\n",
    "\n",
    "def get_judgment(prompt, feedback):\n",
    "    llm_client = LLMAPIFactory(PATH_TO_SECRETS).get()\n",
    "    judgment = make_llm_request(\n",
    "        llm_client,\n",
    "        [{'role': 'system', 'content': prompt.format(FEEDBACK=feedback)}],\n",
    "        'gpt-4-1106-preview',\n",
    "        temperature= 0.0,\n",
    "        max_tokens = 4000,\n",
    "        response_format = {'type': \"json_object\"},\n",
    "        retries = 3,\n",
    "    )\n",
    "    return judgment\n",
    "\n",
    "# Example of running the function\n",
    "#judgment = get_judgment(CODE_PROMPT, gpt_reviews_df.iloc[0]['code_feedback'])\n",
    "#judgment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(reviews, issue_level):\n",
    "    df = notebook_reviews_to_df(filter(None, reviews), issue_level)\n",
    "    for i, (index, row) in enumerate(df.iterrows()):\n",
    "        print(f\"Row {i + 1}/{len(df)} for {issue_level}:\")\n",
    "        code_judgment = get_judgment(CODE_PROMPT, row['code_feedback'])\n",
    "        lang_judgment = get_judgment(LANG_PROMPT, row['lang_feedback'])\n",
    "        df.loc[index, 'code_judgment'] = code_judgment['judgment']\n",
    "        df.loc[index, 'code_judgment_score'] = code_judgment['score']\n",
    "        df.loc[index, 'lang_judgment'] = lang_judgment['judgment']\n",
    "        df.loc[index, 'lang_judgment_score'] = lang_judgment['score']\n",
    "        df.loc[index, 'total_score'] = row['code_score'] + row['lang_score']\n",
    "        print(f\"Code Score: {row['code_score']}, Lang Score: {row['lang_score']}, Total Score: {df.loc[index, 'total_score']}\")\n",
    "        print(f\"Code Judgment: {df.loc[index, 'code_judgment']}\")\n",
    "        print(f\"Full Code Score: {df.loc[index, 'code_judgment_score']}\")\n",
    "        print(f\"Language Judgment: {df.loc[index, 'lang_judgment']}\")\n",
    "        print(f\"Full language Score: {df.loc[index, 'lang_judgment_score']}\")\n",
    "        print('='*60)\n",
    "    return df\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "# Create a counter and a lock outside the function\n",
    "\n",
    "def process_row(index, row, total_reviews):\n",
    "    global counter, results\n",
    "    try:\n",
    "        code_judgment = get_judgment(CODE_PROMPT, row['code_feedback'])\n",
    "        lang_judgment = get_judgment(LANG_PROMPT, row['lang_feedback'])\n",
    "        results[index] = {\n",
    "            'code_judgment': code_judgment.get('judgment', None),\n",
    "            'code_judgment_score': code_judgment.get('score', None),\n",
    "            'lang_judgment': lang_judgment.get('judgment', None),\n",
    "            'lang_judgment_score': lang_judgment.get('score', None),\n",
    "            'total_score': row['code_score'] + row['lang_score']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {str(e)}\")\n",
    "    with counter_lock:\n",
    "        counter += 1\n",
    "        print(f\"Processed {counter}/{total_reviews} rows\")\n",
    "\n",
    "def process_dataframe_parallel(reviews, issue_level, max_workers=20):\n",
    "    global counter\n",
    "    counter = 0  # Reset the counter before starting\n",
    "    df = notebook_reviews_to_df(filter(None, reviews), issue_level)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i, (index, row) in enumerate(df.iterrows()):\n",
    "            executor.submit(process_row, index, row, len(reviews))\n",
    "    # Update the dataframe with the results after all rows have been processed\n",
    "    for index, result in results.items():\n",
    "        for key, value in result.items():\n",
    "            df.loc[index, key] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0\n",
    "counter_lock = threading.Lock()\n",
    "results = {}\n",
    "\n",
    "df_reviews = process_dataframe_parallel(reviews, ISSUE_LEVEL, max_workers=20)\n",
    "df_reviews.sort_values(by=['code_score', 'lang_score'], inplace=True, ascending=False)\n",
    "df_reviews.to_csv(DATA_DIR + f'report_{ISSUE_LEVEL}.csv')\n",
    "import pickle\n",
    "\n",
    "with open(DATA_DIR + '2step_reviews_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df_reviews, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms(df, issue_level):\n",
    "    plt.figure(figsize=(21, 7))\n",
    "\n",
    "    df['code_judgment_score'] = df['code_judgment_score'].astype(float)\n",
    "    df['lang_judgment_score'] = df['lang_judgment_score'].astype(float)\n",
    "\n",
    "    # Plotting histogram for code scores\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(df['code_score'], bins=range(1, 7), align='left', color='skyblue', edgecolor='black')\n",
    "    plt.title('Histogram of AVG Code Scores')\n",
    "    plt.xlabel('Code Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plotting histogram for language scores\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(df['lang_score'], bins=range(1, 7), align='left', color='lightgreen', edgecolor='black')\n",
    "    plt.title('Histogram of AVG Language Scores')\n",
    "    plt.xlabel('Language Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plotting histogram for code judgment scores\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(df['code_judgment_score'], bins=range(1, 7), align='left', color='lightcoral', edgecolor='black')\n",
    "    plt.title('Histogram of Code Judgment Scores' + f' for {issue_level} issues')\n",
    "    plt.xlabel('Code Judgment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plotting histogram for language judgment scores\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(df['lang_judgment_score'], bins=range(1, 7), align='left', color='lightcoral', edgecolor='black')\n",
    "    plt.title('Histogram of Language Judgment Scores' + f' for {issue_level} issues')\n",
    "    plt.xlabel('Language Judgment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(df_reviews, ISSUE_LEVEL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
