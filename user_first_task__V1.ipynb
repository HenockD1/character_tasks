{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Like Quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Existing Questons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.gdrive_api.utils import extract_questions, find_and_load_all_problems, extract_questions_by_topic\n",
    "\n",
    "# parent_folder_path = './notebooks'\n",
    "\n",
    "# all_problems = find_and_load_all_problems(parent_folder_path)\n",
    "# questions = extract_questions(all_problems)\n",
    "# existing_questions = list(questions)\n",
    "# existing_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gdrive_api.utils import extract_questions, find_and_load_all_problems, extract_questions_by_topic\n",
    "\n",
    "parent_dir = \"./notebooks\"\n",
    "all_problems = find_and_load_all_problems(parent_dir)\n",
    "questions_grouped_by_topic = extract_questions_by_topic(all_problems)\n",
    "\n",
    "# # Printing each topic and its questions\n",
    "# for topic, questions in questions_grouped_by_topic.items():\n",
    "#     print(f\"Topic: {topic}\")\n",
    "#     for question in questions:\n",
    "#         print(f\" - {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_by_topic = extract_questions_by_topic(all_problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of topics: 132\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to crawl the topic hierarchy and retrieve all topics\n",
    "def crawl_keys(d, sep=' > ', prefix=''):\n",
    "    paths = []\n",
    "    for k, v in d.items():\n",
    "        path = prefix + k\n",
    "        if isinstance(v, dict) and len(v.keys()) > 0:\n",
    "            paths.extend(crawl_keys(v, sep, path + sep))\n",
    "        else:\n",
    "            paths.append(path)\n",
    "    return paths\n",
    "\n",
    "# Open the `topic_hierarchy.json` file and retrieve all topics\n",
    "with open('topic_hierarchy.json') as json_file:\n",
    "    topic_hierarchy = json.load(json_file)\n",
    "\n",
    "all_topics = crawl_keys(topic_hierarchy)\n",
    "print(f\"Total number of topics: {len(all_topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Questions to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUESTIONS = 100  # The maximum number of questions\n",
    "generated_questions_count = 0  # Counter for the number of questions generated so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:17<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.gdrive_api.utils import generate_human_like_questions\n",
    "from tqdm import tqdm\n",
    "\n",
    "problems = []\n",
    "\n",
    "questions_grouped_by_topics = extract_questions_by_topic(all_problems)\n",
    "\n",
    "# If the existing questions are not grouped by full topic paths, convert them first\n",
    "questions_grouped_by_topic = {}\n",
    "for topic in all_topics:\n",
    "    questions_grouped_by_topic[topic] = questions_grouped_by_topics.get(topic, set())\n",
    "\n",
    "with tqdm(total=MAX_QUESTIONS) as pbar:\n",
    "    for topic, existing_questions in questions_grouped_by_topics.items():\n",
    "        # Stop if we've reached the max limit\n",
    "        if generated_questions_count >= MAX_QUESTIONS:\n",
    "            break\n",
    "\n",
    "        # Generate questions\n",
    "        questions = generate_human_like_questions(topic, 5, existing_questions)\n",
    "        for question in questions[\"questions\"]:\n",
    "            # If we're at the max, break\n",
    "            if generated_questions_count >= MAX_QUESTIONS:\n",
    "                break\n",
    "            problems.append({\n",
    "                \"metadata\": {\n",
    "                    \"topic\": topic,\n",
    "                    \"type\": \"query\",\n",
    "                    \"difficulty\": \"Easy\",\n",
    "                    \"target_length\": 1\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": question},\n",
    "                ]\n",
    "            })\n",
    "            generated_questions_count += 1  # Increment for each question\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from src.gdrive_api.utils import generate_human_like_questions\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# problems = []\n",
    "\n",
    "# with tqdm(total=MAX_QUESTIONS) as pbar:\n",
    "#     for topic in all_topics:\n",
    "#         # Stop if we've reached the max limit\n",
    "#         if generated_questions_count >= MAX_QUESTIONS:\n",
    "#             break\n",
    "\n",
    "#         existing_questions = questions_grouped_by_topic.get(f'{topic}', set())\n",
    "#         print('existing' + str(existing_questions))\n",
    "#         print('topics' + str(topic))\n",
    "\n",
    "#         # Generate questions\n",
    "#         questions = generate_human_like_questions(topic, 5, existing_questions)\n",
    "#         for question in questions[\"questions\"]:\n",
    "#             # If we're at the max, break\n",
    "#             if generated_questions_count >= MAX_QUESTIONS:\n",
    "#                 break\n",
    "#             problems.append({\n",
    "#                 \"metadata\": {\n",
    "#                     \"topic\": topic,\n",
    "#                     \"type\": \"query\",\n",
    "#                     \"difficulty\": \"Easy\",\n",
    "#                     \"target_length\": 1\n",
    "#                 },\n",
    "#                 \"messages\": [\n",
    "#                     {\"role\": \"user\", \"content\": question},\n",
    "#                 ]\n",
    "#             })\n",
    "#             generated_questions_count += 1  # Increment for each question\n",
    "#             pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:27<00:00,  6.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.gdrive_api.utils import generate_human_like_code_modification_requests\n",
    "\n",
    "generated_questions_count = 0\n",
    "\n",
    "with tqdm(total=MAX_QUESTIONS) as pbar:\n",
    "    for topic, existing_questions in questions_grouped_by_topics.items():\n",
    "        # Stop if we've reached the max limit\n",
    "        if generated_questions_count >= MAX_QUESTIONS:\n",
    "            break\n",
    "        \n",
    "        questions = generate_human_like_code_modification_requests(topic, 3, existing_questions)\n",
    "        for question in questions[\"questions\"]:\n",
    "            # If we're at the max, break\n",
    "            if generated_questions_count >= MAX_QUESTIONS:\n",
    "                break\n",
    "            problems.append({\n",
    "                \"metadata\": {\n",
    "                    \"topic\": topic,\n",
    "                    \"type\": \"modification\",\n",
    "                    \"difficulty\": \"Easy\",\n",
    "                    \"target_length\": 1\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": question},\n",
    "                ]\n",
    "            })\n",
    "            generated_questions_count += 1  # Increment for each question\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_titles = []\n",
    "problem_topic_counts = {}\n",
    "file_path_to_problem = {}\n",
    "for problem in problems:\n",
    "    topic_type = f'{problem[\"metadata\"][\"topic\"].split(\" > \")[-1]}__{problem[\"metadata\"][\"type\"]}'\n",
    "    idx = problem_topic_counts.get(topic_type, 0)\n",
    "    title = f'{topic_type}__{idx}'\n",
    "    problem_titles.append(title)\n",
    "    file_path_to_problem[f\"{title}.ipynb\"] = problem\n",
    "    problem_topic_counts[topic_type] = idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distribution for the number of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Define distribution for the number of turns\n",
    "distribution = {1: 25, 2: 50, 6: 25}\n",
    "\n",
    "# Ensure that the distribution sums up to 100%\n",
    "assert sum(distribution.values()) == 100, \"The distribution must sum to 100%\"\n",
    "\n",
    "def weighted_choice(choices):\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        if upto + weight >= r:\n",
    "            return choice\n",
    "        upto += weight\n",
    "\n",
    "def select_number_of_turns(distribution):\n",
    "    # Normalize distribution values to probabilities\n",
    "    total = sum(distribution.values())\n",
    "    normalized_distribution = {k: v/total for k, v in distribution.items()}\n",
    "    turns = weighted_choice(normalized_distribution.items())\n",
    "    return turns\n",
    "\n",
    "for p, t in zip(problems, problem_titles):\n",
    "    # Determine the parity based on the last digit in the title\n",
    "    title_number = int(re.search(r'\\d+$', t).group())  # Extract the last number from the title\n",
    "\n",
    "    turns = select_number_of_turns(distribution)\n",
    "    \n",
    "    parity = title_number % 2\n",
    "\n",
    "    # Create a new notebook\n",
    "    notebook = new_notebook()\n",
    "\n",
    "    # Add metadata\n",
    "    metadata = f\"\"\"# Metadata\n",
    "\n",
    "**Python Topics** - {p[\"metadata\"][\"topic\"]}\n",
    "\n",
    "**Type** - {p[\"metadata\"][\"type\"]}\n",
    "\n",
    "**Target Number of Turns (User + Assistant)** - {turns}\n",
    "\"\"\"\n",
    "    metadata_cell = new_markdown_cell(metadata)\n",
    "    notebook.cells.append(metadata_cell)\n",
    "\n",
    "    # Add conversation header\n",
    "    conversation_header = \"# Conversation\"\n",
    "    conversation_header_cell = new_markdown_cell(conversation_header)\n",
    "    notebook.cells.append(conversation_header_cell)\n",
    "\n",
    "    # Append conversation messages\n",
    "    title = \"**User**\" if parity else \"**Assistant**\"\n",
    "    for message in p[\"messages\"]:\n",
    "        msg_content = f\"\"\"{title}\n",
    "\n",
    "{message[\"content\"]}\n",
    "\"\"\"\n",
    "        conversation_message_cell = new_markdown_cell(msg_content)\n",
    "        notebook.cells.append(conversation_message_cell)\n",
    "\n",
    "    # Save the notebook\n",
    "    notebook_path = f'notebooks/test/{t}.ipynb'\n",
    "     \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(notebook_path))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "     \n",
    "    with open(notebook_path, 'w') as f:\n",
    "        nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biased_normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns2\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns2\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns2\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns4\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns2\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns6\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns4\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns4\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns4\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns3\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns4\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns4\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns6\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns3\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns2\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns1\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns6\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns6\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns5\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns8\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns3\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns4\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns6\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns1\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns4\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns2\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns2\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns4\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns1\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns7\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns1\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns6\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns7\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns4\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns1\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns7\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns4\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns6\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns5\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns5\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns4\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns7\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns4\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns5\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns5\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns6\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns2\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns1\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns2\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns7\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns2\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns5\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns1\n",
      "algorithms.by_data_structure.arrays\n",
      "Topic: algorithms.by_data_structure.arrays, Percentile: 0.03787878787878788\n",
      "turns3\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.linked_lists\n",
      "Topic: algorithms.by_data_structure.linked_lists, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns5\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.stacks\n",
      "Topic: algorithms.by_data_structure.stacks, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_data_structure.queues\n",
      "Topic: algorithms.by_data_structure.queues, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns7\n",
      "algorithms.by_data_structure.trees\n",
      "Topic: algorithms.by_data_structure.trees, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns2\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns1\n",
      "algorithms.by_data_structure.graphs\n",
      "Topic: algorithms.by_data_structure.graphs, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_data_structure.hash_tables\n",
      "Topic: algorithms.by_data_structure.hash_tables, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_data_structure.heaps\n",
      "Topic: algorithms.by_data_structure.heaps, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns4\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns7\n",
      "algorithms.by_data_structure.strings\n",
      "Topic: algorithms.by_data_structure.strings, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns6\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns5\n",
      "algorithms.by_data_structure.advanced_data_structures\n",
      "Topic: algorithms.by_data_structure.advanced_data_structures, Percentile: 0.9886363636363636\n",
      "turns4\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns3\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns5\n",
      "algorithms.by_topic.dynamic_programming\n",
      "Topic: algorithms.by_topic.dynamic_programming, Percentile: 0.9886363636363636\n",
      "turns2\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns3\n",
      "algorithms.by_topic.famous_algorithms\n",
      "Topic: algorithms.by_topic.famous_algorithms, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns1\n",
      "algorithms.by_topic.greedy_algorithms\n",
      "Topic: algorithms.by_topic.greedy_algorithms, Percentile: 0.7462121212121212\n",
      "turns1\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns8\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.recursion\n",
      "Topic: algorithms.by_topic.recursion, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.searching\n",
      "Topic: algorithms.by_topic.searching, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_topic.sorting\n",
      "Topic: algorithms.by_topic.sorting, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.math\n",
      "Topic: algorithms.by_topic.math, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.bit_manipulation\n",
      "Topic: algorithms.by_topic.bit_manipulation, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns9\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.geometry\n",
      "Topic: algorithms.by_topic.geometry, Percentile: 0.7462121212121212\n",
      "turns3\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns4\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns1\n",
      "algorithms.by_topic.probability\n",
      "Topic: algorithms.by_topic.probability, Percentile: 0.7462121212121212\n",
      "turns6\n",
      "algorithms.by_topic.game_theory\n",
      "Topic: algorithms.by_topic.game_theory, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.game_theory\n",
      "Topic: algorithms.by_topic.game_theory, Percentile: 0.9015151515151515\n",
      "turns1\n",
      "algorithms.by_topic.game_theory\n",
      "Topic: algorithms.by_topic.game_theory, Percentile: 0.9015151515151515\n",
      "turns2\n",
      "algorithms.by_topic.divide_and_conquer\n",
      "Topic: algorithms.by_topic.divide_and_conquer, Percentile: 0.9015151515151515\n",
      "turns6\n",
      "algorithms.by_topic.divide_and_conquer\n",
      "Topic: algorithms.by_topic.divide_and_conquer, Percentile: 0.9015151515151515\n",
      "turns7\n",
      "algorithms.by_topic.divide_and_conquer\n",
      "Topic: algorithms.by_topic.divide_and_conquer, Percentile: 0.9015151515151515\n",
      "turns3\n",
      "algorithms.by_topic.backtracking\n",
      "Topic: algorithms.by_topic.backtracking, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.backtracking\n",
      "Topic: algorithms.by_topic.backtracking, Percentile: 0.9015151515151515\n",
      "turns1\n",
      "algorithms.by_topic.backtracking\n",
      "Topic: algorithms.by_topic.backtracking, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.counting\n",
      "Topic: algorithms.by_topic.counting, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.counting\n",
      "Topic: algorithms.by_topic.counting, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.counting\n",
      "Topic: algorithms.by_topic.counting, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.statistics\n",
      "Topic: algorithms.by_topic.statistics, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.statistics\n",
      "Topic: algorithms.by_topic.statistics, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.statistics\n",
      "Topic: algorithms.by_topic.statistics, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.combinatorics\n",
      "Topic: algorithms.by_topic.combinatorics, Percentile: 0.9015151515151515\n",
      "turns5\n",
      "algorithms.by_topic.combinatorics\n",
      "Topic: algorithms.by_topic.combinatorics, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_topic.combinatorics\n",
      "Topic: algorithms.by_topic.combinatorics, Percentile: 0.9015151515151515\n",
      "turns4\n",
      "algorithms.by_topic.union_find\n",
      "Topic: algorithms.by_topic.union_find, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "algorithms.by_topic.union_find\n",
      "Topic: algorithms.by_topic.union_find, Percentile: 0.5378787878787878\n",
      "turns3\n",
      "algorithms.by_topic.union_find\n",
      "Topic: algorithms.by_topic.union_find, Percentile: 0.5378787878787878\n",
      "turns5\n",
      "algorithms.by_topic.sliding_window\n",
      "Topic: algorithms.by_topic.sliding_window, Percentile: 0.3143939393939394\n",
      "turns8\n",
      "algorithms.by_topic.sliding_window\n",
      "Topic: algorithms.by_topic.sliding_window, Percentile: 0.3143939393939394\n",
      "turns2\n",
      "algorithms.by_topic.sliding_window\n",
      "Topic: algorithms.by_topic.sliding_window, Percentile: 0.3143939393939394\n",
      "turns1\n",
      "algorithms.by_topic.path_finding\n",
      "Topic: algorithms.by_topic.path_finding, Percentile: 0.7462121212121212\n",
      "turns2\n",
      "algorithms.by_topic.path_finding\n",
      "Topic: algorithms.by_topic.path_finding, Percentile: 0.7462121212121212\n",
      "turns5\n",
      "algorithms.by_topic.path_finding\n",
      "Topic: algorithms.by_topic.path_finding, Percentile: 0.7462121212121212\n",
      "turns5\n",
      "algorithms.by_topic.two_pointers\n",
      "Topic: algorithms.by_topic.two_pointers, Percentile: 0.14015151515151514\n",
      "turns2\n",
      "algorithms.by_topic.two_pointers\n",
      "Topic: algorithms.by_topic.two_pointers, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_topic.two_pointers\n",
      "Topic: algorithms.by_topic.two_pointers, Percentile: 0.14015151515151514\n",
      "turns1\n",
      "algorithms.by_topic.sampling\n",
      "Topic: algorithms.by_topic.sampling, Percentile: 0.5378787878787878\n",
      "turns1\n",
      "algorithms.by_topic.sampling\n",
      "Topic: algorithms.by_topic.sampling, Percentile: 0.5378787878787878\n",
      "turns7\n",
      "algorithms.by_topic.sampling\n",
      "Topic: algorithms.by_topic.sampling, Percentile: 0.5378787878787878\n",
      "turns2\n",
      "python_language_and_scripting.basic_python_syntax\n",
      "Topic: python_language_and_scripting.basic_python_syntax, Percentile: 0.007575757575757576\n",
      "turns3\n",
      "python_language_and_scripting.basic_python_syntax\n",
      "Topic: python_language_and_scripting.basic_python_syntax, Percentile: 0.007575757575757576\n",
      "turns1\n",
      "python_language_and_scripting.basic_python_syntax\n",
      "Topic: python_language_and_scripting.basic_python_syntax, Percentile: 0.007575757575757576\n",
      "turns2\n",
      "python_language_and_scripting.functions_and_modules\n",
      "Topic: python_language_and_scripting.functions_and_modules, Percentile: 0.03787878787878788\n",
      "turns1\n",
      "python_language_and_scripting.functions_and_modules\n",
      "Topic: python_language_and_scripting.functions_and_modules, Percentile: 0.03787878787878788\n",
      "turns1\n",
      "python_language_and_scripting.functions_and_modules\n",
      "Topic: python_language_and_scripting.functions_and_modules, Percentile: 0.03787878787878788\n",
      "turns1\n",
      "python_language_and_scripting.file_handling\n",
      "Topic: python_language_and_scripting.file_handling, Percentile: 0.14015151515151514\n",
      "turns5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nbformat\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "def flatten_complexity_scores(data, key_prefix='', scores_dict=None):\n",
    "    if scores_dict is None:\n",
    "        scores_dict = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            if 'complexity' in value:\n",
    "                scores_dict[f'{key_prefix}.{key}'.strip('.')] = value['complexity']\n",
    "            else:\n",
    "                new_prefix = f'{key_prefix}.{key}'.strip('.')\n",
    "                flatten_complexity_scores(value, new_prefix, scores_dict)\n",
    "        elif isinstance(value, int):\n",
    "            scores_dict[key_prefix] = value\n",
    "    return scores_dict\n",
    "\n",
    "with open('./topic_dist.json') as f:\n",
    "    complexity_data = json.load(f)\n",
    "\n",
    "# Flatten the complexity data into a dictionary\n",
    "complexity_scores = flatten_complexity_scores(complexity_data)\n",
    "\n",
    "# Sort the complexity scores in descending order\n",
    "sorted_by_complexity = dict(sorted(complexity_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Calculate the percentiles based on the sorted complexity scores\n",
    "# Higher complexity will have higher rank and thus higher percentile\n",
    "complexity_ranks = rankdata(list(sorted_by_complexity.values()), method='average')\n",
    "topic_percentiles = {topic: rank / len(complexity_scores) for topic, rank in zip(sorted_by_complexity.keys(), complexity_ranks)}\n",
    "\n",
    "def biased_normal_distribution(input_number):\n",
    "    # Constants for the distribution\n",
    "    min_val, max_val = 1, 15\n",
    "    std_dev = 2.5  # Fixed standard deviation\n",
    "\n",
    "    # Base mean for input_number 50 (biased towards lower end)\n",
    "    base_mean = 1\n",
    "\n",
    "    # Adjust the mean based on the input number (linear mapping)\n",
    "    mean = np.interp(input_number, [0, 100], [min_val, max_val])\n",
    "    \n",
    "    # Adjust mean for the bias towards lower numbers\n",
    "    adjusted_mean = (mean + 3*base_mean) / 5\n",
    "\n",
    "    # Sample from a normal distribution\n",
    "    while True:\n",
    "        sample = np.random.normal(adjusted_mean, std_dev)\n",
    "        if min_val <= sample <= max_val:\n",
    "            break\n",
    "\n",
    "    return int(sample)\n",
    "\n",
    "# # Function to calculate the number of turns based on percentile\n",
    "# # Higher complexity (higher percentile) results in more turns\n",
    "# def calculate_turns(percentile, max_turns=10):\n",
    "#     turns = max(1, round(percentile * max_turns))\n",
    "#     return turns\n",
    "\n",
    "for p, t in zip(problems, problem_titles):\n",
    "\n",
    "    title_number = int(re.search(r'\\d+$', t).group())  # Extract the last number from the title\n",
    "    \n",
    "    parity = title_number % 2\n",
    "\n",
    "    metadata_topic_hierarchy = p[\"metadata\"].get(\"topic\") \n",
    "    normalized_topic = metadata_topic_hierarchy.replace(\" > \", \".\")\n",
    "    print(normalized_topic)\n",
    "\n",
    "    # Retrieve the percentile using the normalized topic string\n",
    "    percentile = topic_percentiles.get(normalized_topic, 0)\n",
    "    print(f\"Topic: {normalized_topic}, Percentile: {percentile}\")\n",
    "\n",
    "    # Calculate turns using the biased_normal_distribution function\n",
    "    turns = biased_normal_distribution(percentile * 100)  # Percentile scaled to 0-100 \n",
    "    print('turns'+str(turns))\n",
    "    \n",
    "    # Create a new notebook\n",
    "    notebook = new_notebook()\n",
    "    # Add metadata\n",
    "    metadata = f\"\"\"# Metadata\n",
    "\n",
    "**Python Topics** - {p[\"metadata\"][\"topic\"]}\n",
    "\n",
    "**Type** - {p[\"metadata\"][\"type\"]}\n",
    "\n",
    "**Target Number of Turns (User + Assistant)** - {turns}\n",
    "\"\"\"\n",
    "    metadata_cell = new_markdown_cell(metadata)\n",
    "    notebook.cells.append(metadata_cell)\n",
    "\n",
    "    # Add conversation header\n",
    "    conversation_header = \"# Conversation\"\n",
    "    conversation_header_cell = new_markdown_cell(conversation_header)\n",
    "    notebook.cells.append(conversation_header_cell)\n",
    "\n",
    "    # Append conversation messages\n",
    "    title = \"**User**\" if parity else \"**Assistant**\"\n",
    "    for message in p[\"messages\"]:\n",
    "        msg_content = f\"\"\"{title}\n",
    "\n",
    "{message[\"content\"]}\n",
    "\"\"\"\n",
    "        conversation_message_cell = new_markdown_cell(msg_content)\n",
    "        notebook.cells.append(conversation_message_cell)\n",
    "\n",
    "    # Save the notebook\n",
    "    notebook_path = f'notebooks/test/{t}.ipynb'\n",
    "     \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(notebook_path))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "     \n",
    "    with open(notebook_path, 'w') as f:\n",
    "        nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithms_by_data_structure_advanced_data_structures: Complexity 8\n",
      "algorithms_by_topic_dynamic_programming: Complexity 8\n",
      "python_language_and_scripting_python_c_extensions: Complexity 8\n",
      "deep_learning_backpropagation_understanding: Complexity 8\n",
      "algorithms_by_data_structure_graphs: Complexity 7\n",
      "algorithms_by_topic_famous_algorithms: Complexity 7\n",
      "algorithms_by_topic_game_theory: Complexity 7\n",
      "algorithms_by_topic_divide_and_conquer: Complexity 7\n",
      "algorithms_by_topic_backtracking: Complexity 7\n",
      "algorithms_by_topic_combinatorics: Complexity 7\n",
      "python_language_and_scripting_metaclasses_and_class_factories: Complexity 7\n",
      "python_language_and_scripting_memory_management_and_python_internals: Complexity 7\n",
      "python_language_and_scripting_advanced_networking: Complexity 7\n",
      "python_language_and_scripting_cython_and_PyPy: Complexity 7\n",
      "python_language_and_scripting_parallel_programming: Complexity 7\n",
      "unit_testing_methodology_test_ai_and_ml_models: Complexity 7\n",
      "web_development_web_security: Complexity 7\n",
      "ml_principles_neural_networks_implementation: Complexity 7\n",
      "deep_learning_optimization_algorithms: Complexity 7\n",
      "deep_learning_convolutional_neural_networks: Complexity 7\n",
      "deep_learning_recurrent_neural_networks: Complexity 7\n",
      "database_sql_query_optimization: Complexity 7\n",
      "database_database_security_best_practices: Complexity 7\n",
      "algorithms_by_topic_greedy_algorithms: Complexity 6\n",
      "algorithms_by_topic_geometry: Complexity 6\n",
      "algorithms_by_topic_probability: Complexity 6\n",
      "algorithms_by_topic_path_finding: Complexity 6\n",
      "python_language_and_scripting_descriptors: Complexity 6\n",
      "python_language_and_scripting_dynamic_programming_and_closures: Complexity 6\n",
      "python_language_and_scripting_functional_programming_features: Complexity 6\n",
      "unit_testing_methodology_performance_testing: Complexity 6\n",
      "unit_testing_methodology_security_testing: Complexity 6\n",
      "unit_testing_methodology_testing_asynchronous_code: Complexity 6\n",
      "unit_testing_methodology_data_quality_tests: Complexity 6\n",
      "data_analysis_time_series_analysis: Complexity 6\n",
      "ml_principles_cross_validation_application: Complexity 6\n",
      "ml_principles_hyperparameter_tuning: Complexity 6\n",
      "deep_learning_neural_network_concepts: Complexity 6\n",
      "deep_learning_loss_functions: Complexity 6\n",
      "deep_learning_regularization_techniques: Complexity 6\n",
      "deep_learning_transfer_learning: Complexity 6\n",
      "database_relational_database_design: Complexity 6\n",
      "database_transaction_management: Complexity 6\n",
      "database_indexing_and_search_performance: Complexity 6\n",
      "database_data_replication_strategies: Complexity 6\n",
      "algorithms_by_data_structure_trees: Complexity 5\n",
      "algorithms_by_data_structure_heaps: Complexity 5\n",
      "algorithms_by_topic_recursion: Complexity 5\n",
      "algorithms_by_topic_math: Complexity 5\n",
      "algorithms_by_topic_bit_manipulation: Complexity 5\n",
      "algorithms_by_topic_counting: Complexity 5\n",
      "algorithms_by_topic_statistics: Complexity 5\n",
      "algorithms_by_topic_union_find: Complexity 5\n",
      "algorithms_by_topic_sampling: Complexity 5\n",
      "python_language_and_scripting_decorators: Complexity 5\n",
      "python_language_and_scripting_generators_and_iterators: Complexity 5\n",
      "python_language_and_scripting_context_managers: Complexity 5\n",
      "python_language_and_scripting_advanced_exception_handling: Complexity 5\n",
      "python_language_and_scripting_packaging: Complexity 5\n",
      "unit_testing_methodology_test_driven_development: Complexity 5\n",
      "unit_testing_methodology_mocking_and_patching: Complexity 5\n",
      "unit_testing_methodology_best_practices_in_writing_test_cases: Complexity 5\n",
      "unit_testing_methodology_testing_database_interaction: Complexity 5\n",
      "web_development_web_performance: Complexity 5\n",
      "web_development_web_services: Complexity 5\n",
      "web_development_web_development_best_practices: Complexity 5\n",
      "data_analysis_data_visualization: Complexity 5\n",
      "data_analysis_pivot_tables_usage: Complexity 5\n",
      "ml_principles_knn_classification: Complexity 5\n",
      "ml_principles_model_accuracy_assessment: Complexity 5\n",
      "ml_principles_feature_selection_techniques: Complexity 5\n",
      "ml_principles_decision_trees_usage: Complexity 5\n",
      "deep_learning_activation_functions: Complexity 5\n",
      "deep_learning_deep_learning_frameworks: Complexity 5\n",
      "database_no_sql_database_concepts: Complexity 5\n",
      "database_database_normalization: Complexity 5\n",
      "database_database_backup_and_recovery: Complexity 5\n",
      "database_cloud_database_solutions: Complexity 5\n",
      "algorithms_by_data_structure_hash_tables: Complexity 4\n",
      "algorithms_by_data_structure_strings: Complexity 4\n",
      "algorithms_by_topic_sorting: Complexity 4\n",
      "algorithms_by_topic_sliding_window: Complexity 4\n",
      "python_language_and_scripting_object_oriented_programming: Complexity 4\n",
      "python_language_and_scripting_data_handling_manipulation: Complexity 4\n",
      "python_language_and_scripting_pythonic_code: Complexity 4\n",
      "python_language_and_scripting_python_standard_library: Complexity 4\n",
      "python_language_and_scripting_data_serialization_and_marshaling: Complexity 4\n",
      "unit_testing_methodology_integration_testing: Complexity 4\n",
      "unit_testing_methodology_code_coverage: Complexity 4\n",
      "web_development_python_frameworks: Complexity 4\n",
      "web_development_web_servers: Complexity 4\n",
      "web_development_web_design: Complexity 4\n",
      "web_development_web_development_tools: Complexity 4\n",
      "web_development_web_development_interview_questions: Complexity 4\n",
      "data_analysis_data_cleaning: Complexity 4\n",
      "data_analysis_data_grouping_aggregation: Complexity 4\n",
      "data_analysis_data_merging_joining: Complexity 4\n",
      "data_analysis_sql_query_with_pandas: Complexity 4\n",
      "data_analysis_duplicate_data_identification: Complexity 4\n",
      "ml_principles_data_normalization_scaling: Complexity 4\n",
      "ml_principles_linear_regression_implementation: Complexity 4\n",
      "debugging_and_tracing: Complexity 4\n",
      "high_school_math: Complexity 4\n",
      "high_school_physics: Complexity 4\n",
      "algorithms_by_data_structure_linked_lists: Complexity 3\n",
      "algorithms_by_data_structure_stacks: Complexity 3\n",
      "algorithms_by_data_structure_queues: Complexity 3\n",
      "algorithms_by_topic_searching: Complexity 3\n",
      "algorithms_by_topic_two_pointers: Complexity 3\n",
      "python_language_and_scripting_file_handling: Complexity 3\n",
      "python_language_and_scripting_scripting_and_automation: Complexity 3\n",
      "python_language_and_scripting_modules_and_packages: Complexity 3\n",
      "python_language_and_scripting_type_hinting: Complexity 3\n",
      "python_language_and_scripting_virtual_environments: Complexity 3\n",
      "python_language_and_scripting_logging: Complexity 3\n",
      "unit_testing_methodology_testing_frameworks: Complexity 3\n",
      "web_development_web_scraping: Complexity 3\n",
      "web_development_web_crawling: Complexity 3\n",
      "web_development_web_development_trends: Complexity 3\n",
      "data_analysis_json_parsing: Complexity 3\n",
      "ml_principles_train_test_split: Complexity 3\n",
      "high_school_programming: Complexity 3\n",
      "high_school_accounting: Complexity 3\n",
      "high_school_economics: Complexity 3\n",
      "algorithms_by_data_structure_arrays: Complexity 2\n",
      "python_language_and_scripting_functions_and_modules: Complexity 2\n",
      "python_language_and_scripting_python_2_vs_3: Complexity 2\n",
      "unit_testing_methodology_unit_testing_basics: Complexity 2\n",
      "web_development_fundamentals: Complexity 2\n",
      "data_analysis_csv_handling: Complexity 2\n",
      "everyday_use_cases: Complexity 2\n",
      "python_language_and_scripting_basic_python_syntax: Complexity 1\n"
     ]
    }
   ],
   "source": [
    "with open('./topic_dist.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the data structure, creating a list of (category, complexity) tuples\n",
    "def flatten_data(data, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in data.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            if 'complexity' in v:\n",
    "                items.append((new_key, v['complexity']))\n",
    "            else:\n",
    "                items.extend(flatten_data(v, new_key, sep=sep))\n",
    "    return items\n",
    "\n",
    "# Flatten the provided JSON data\n",
    "flattened_data = flatten_data(data)\n",
    "\n",
    "# Now we sort the flattened data by complexity in descending order\n",
    "sorted_data = sorted(flattened_data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for item in sorted_data:\n",
    "    print(f\"{item[0]}: Complexity {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('notebooks/test/problems.json', 'w') as f:\n",
    "    json.dump(problems, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/test/problems.json') as f:\n",
    "    problems = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Processing directory .: 1 of 0 in total.\n",
      "Uploading file 1 of 41 in '.', 1 of 41 in total.\n",
      "Uploading new file 'arrays__modification__0.ipynb'.\n",
      "File 'arrays__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'arrays__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 2 of 41 in '.', 2 of 41 in total.\n",
      "Uploading new file 'arrays__modification__1.ipynb'.\n",
      "File 'arrays__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'arrays__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 3 of 41 in '.', 3 of 41 in total.\n",
      "Uploading new file 'arrays__modification__2.ipynb'.\n",
      "File 'arrays__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'arrays__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__modification__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 4 of 41 in '.', 4 of 41 in total.\n",
      "Replacing existing file 'arrays__query__0.ipynb' with the new version.\n",
      "File 'arrays__query__0.ipynb' has been replaced.\n",
      "Uploaded 'arrays__query__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__query__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 5 of 41 in '.', 5 of 41 in total.\n",
      "Replacing existing file 'arrays__query__1.ipynb' with the new version.\n",
      "File 'arrays__query__1.ipynb' has been replaced.\n",
      "Uploaded 'arrays__query__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__query__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 6 of 41 in '.', 6 of 41 in total.\n",
      "Replacing existing file 'arrays__query__2.ipynb' with the new version.\n",
      "File 'arrays__query__2.ipynb' has been replaced.\n",
      "Uploaded 'arrays__query__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__query__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 7 of 41 in '.', 7 of 41 in total.\n",
      "Replacing existing file 'arrays__query__3.ipynb' with the new version.\n",
      "File 'arrays__query__3.ipynb' has been replaced.\n",
      "Uploaded 'arrays__query__3.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__query__3.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 8 of 41 in '.', 8 of 41 in total.\n",
      "Replacing existing file 'arrays__query__4.ipynb' with the new version.\n",
      "File 'arrays__query__4.ipynb' has been replaced.\n",
      "Uploaded 'arrays__query__4.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "arrays__query__4.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 9 of 41 in '.', 9 of 41 in total.\n",
      "Uploading new file 'graphs__modification__0.ipynb'.\n",
      "File 'graphs__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'graphs__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "graphs__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 10 of 41 in '.', 10 of 41 in total.\n",
      "Uploading new file 'graphs__modification__1.ipynb'.\n",
      "File 'graphs__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'graphs__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "graphs__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 11 of 41 in '.', 11 of 41 in total.\n",
      "Uploading new file 'graphs__modification__2.ipynb'.\n",
      "File 'graphs__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'graphs__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "graphs__modification__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 12 of 41 in '.', 12 of 41 in total.\n",
      "Uploading new file 'hash_tables__modification__0.ipynb'.\n",
      "File 'hash_tables__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'hash_tables__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "hash_tables__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 13 of 41 in '.', 13 of 41 in total.\n",
      "Uploading new file 'hash_tables__modification__1.ipynb'.\n",
      "File 'hash_tables__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'hash_tables__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "hash_tables__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 14 of 41 in '.', 14 of 41 in total.\n",
      "Uploading new file 'linked_lists__modification__0.ipynb'.\n",
      "File 'linked_lists__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'linked_lists__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 15 of 41 in '.', 15 of 41 in total.\n",
      "Uploading new file 'linked_lists__modification__1.ipynb'.\n",
      "File 'linked_lists__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'linked_lists__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 16 of 41 in '.', 16 of 41 in total.\n",
      "Uploading new file 'linked_lists__modification__2.ipynb'.\n",
      "File 'linked_lists__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'linked_lists__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__modification__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 17 of 41 in '.', 17 of 41 in total.\n",
      "Replacing existing file 'linked_lists__query__0.ipynb' with the new version.\n",
      "File 'linked_lists__query__0.ipynb' has been replaced.\n",
      "Uploaded 'linked_lists__query__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__query__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 18 of 41 in '.', 18 of 41 in total.\n",
      "Replacing existing file 'linked_lists__query__1.ipynb' with the new version.\n",
      "File 'linked_lists__query__1.ipynb' has been replaced.\n",
      "Uploaded 'linked_lists__query__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__query__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 19 of 41 in '.', 19 of 41 in total.\n",
      "Replacing existing file 'linked_lists__query__2.ipynb' with the new version.\n",
      "File 'linked_lists__query__2.ipynb' has been replaced.\n",
      "Uploaded 'linked_lists__query__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__query__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 20 of 41 in '.', 20 of 41 in total.\n",
      "Replacing existing file 'linked_lists__query__3.ipynb' with the new version.\n",
      "File 'linked_lists__query__3.ipynb' has been replaced.\n",
      "Uploaded 'linked_lists__query__3.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__query__3.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 21 of 41 in '.', 21 of 41 in total.\n",
      "Replacing existing file 'linked_lists__query__4.ipynb' with the new version.\n",
      "File 'linked_lists__query__4.ipynb' has been replaced.\n",
      "Uploaded 'linked_lists__query__4.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "linked_lists__query__4.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 22 of 41 in '.', 22 of 41 in total.\n",
      "Replacing existing file 'problems.json' with the new version.\n",
      "File 'problems.json' has been replaced.\n",
      "Uploaded 'problems.json' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "problems.json\n",
      "==========================================================================================\n",
      "Uploading file 23 of 41 in '.', 23 of 41 in total.\n",
      "Uploading new file 'queues__modification__0.ipynb'.\n",
      "File 'queues__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'queues__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 24 of 41 in '.', 24 of 41 in total.\n",
      "Uploading new file 'queues__modification__1.ipynb'.\n",
      "File 'queues__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'queues__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 25 of 41 in '.', 25 of 41 in total.\n",
      "Uploading new file 'queues__modification__2.ipynb'.\n",
      "File 'queues__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'queues__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__modification__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 26 of 41 in '.', 26 of 41 in total.\n",
      "Replacing existing file 'queues__query__0.ipynb' with the new version.\n",
      "File 'queues__query__0.ipynb' has been replaced.\n",
      "Uploaded 'queues__query__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__query__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 27 of 41 in '.', 27 of 41 in total.\n",
      "Replacing existing file 'queues__query__1.ipynb' with the new version.\n",
      "File 'queues__query__1.ipynb' has been replaced.\n",
      "Uploaded 'queues__query__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__query__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 28 of 41 in '.', 28 of 41 in total.\n",
      "Replacing existing file 'queues__query__2.ipynb' with the new version.\n",
      "File 'queues__query__2.ipynb' has been replaced.\n",
      "Uploaded 'queues__query__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__query__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 29 of 41 in '.', 29 of 41 in total.\n",
      "Replacing existing file 'queues__query__3.ipynb' with the new version.\n",
      "File 'queues__query__3.ipynb' has been replaced.\n",
      "Uploaded 'queues__query__3.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__query__3.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 30 of 41 in '.', 30 of 41 in total.\n",
      "Replacing existing file 'queues__query__4.ipynb' with the new version.\n",
      "File 'queues__query__4.ipynb' has been replaced.\n",
      "Uploaded 'queues__query__4.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "queues__query__4.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 31 of 41 in '.', 31 of 41 in total.\n",
      "Uploading new file 'stacks__modification__0.ipynb'.\n",
      "File 'stacks__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'stacks__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 32 of 41 in '.', 32 of 41 in total.\n",
      "Uploading new file 'stacks__modification__1.ipynb'.\n",
      "File 'stacks__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'stacks__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 33 of 41 in '.', 33 of 41 in total.\n",
      "Uploading new file 'stacks__modification__2.ipynb'.\n",
      "File 'stacks__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'stacks__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__modification__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 34 of 41 in '.', 34 of 41 in total.\n",
      "Replacing existing file 'stacks__query__0.ipynb' with the new version.\n",
      "File 'stacks__query__0.ipynb' has been replaced.\n",
      "Uploaded 'stacks__query__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__query__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 35 of 41 in '.', 35 of 41 in total.\n",
      "Replacing existing file 'stacks__query__1.ipynb' with the new version.\n",
      "File 'stacks__query__1.ipynb' has been replaced.\n",
      "Uploaded 'stacks__query__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__query__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 36 of 41 in '.', 36 of 41 in total.\n",
      "Replacing existing file 'stacks__query__2.ipynb' with the new version.\n",
      "File 'stacks__query__2.ipynb' has been replaced.\n",
      "Uploaded 'stacks__query__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__query__2.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 37 of 41 in '.', 37 of 41 in total.\n",
      "Replacing existing file 'stacks__query__3.ipynb' with the new version.\n",
      "File 'stacks__query__3.ipynb' has been replaced.\n",
      "Uploaded 'stacks__query__3.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__query__3.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 38 of 41 in '.', 38 of 41 in total.\n",
      "Replacing existing file 'stacks__query__4.ipynb' with the new version.\n",
      "File 'stacks__query__4.ipynb' has been replaced.\n",
      "Uploaded 'stacks__query__4.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "stacks__query__4.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 39 of 41 in '.', 39 of 41 in total.\n",
      "Uploading new file 'trees__modification__0.ipynb'.\n",
      "File 'trees__modification__0.ipynb' has been uploaded.\n",
      "Uploaded 'trees__modification__0.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "trees__modification__0.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 40 of 41 in '.', 40 of 41 in total.\n",
      "Uploading new file 'trees__modification__1.ipynb'.\n",
      "File 'trees__modification__1.ipynb' has been uploaded.\n",
      "Uploaded 'trees__modification__1.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "trees__modification__1.ipynb\n",
      "==========================================================================================\n",
      "Uploading file 41 of 41 in '.', 41 of 41 in total.\n",
      "Uploading new file 'trees__modification__2.ipynb'.\n",
      "File 'trees__modification__2.ipynb' has been uploaded.\n",
      "Uploaded 'trees__modification__2.ipynb' to folder ID '1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'.\n",
      "trees__modification__2.ipynb\n",
      "==========================================================================================\n",
      "============================================================\n",
      "Successfully uploaded 41 files out of 41.\n",
      "Skipped 0 files.\n",
      "Successfully processed 0 directories.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from src.gdrive_api.folder_upload import upload_folder\n",
    "from src.gdrive_api.auth import build_service\n",
    "\n",
    "service = build_service('creds/google__sa.json')\n",
    "# destination_folder_url = \"https://drive.google.com/drive/u/0/folders/1Z1bdYMe2Qmo_vs-OaKDaYIiV3rIqLJH9\"\n",
    "# destination_folder_url = \"https://drive.google.com/drive/u/2/folders/1sfPFHkXYpKyY41V0pfz3Qw3k4VLy5Hvb\"\n",
    "destination_folder_url = 'https://drive.google.com/drive/folders/1T14cHENpx7j2aS4Yb_3WhnXhMql0zq2y'\n",
    "\n",
    "uploaded_files = upload_folder(service, 'notebooks/test', destination_folder_url, force_replace = True, is_url=True)\n",
    "\n",
    "file_path_to_url = {}\n",
    "with tqdm(total=len(uploaded_files)) as pbar:\n",
    "    for file_path, file_url in uploaded_files.items():\n",
    "        if file_url is not None:\n",
    "            drive_id = file_url.split(\"id=\")[-1].split(\"&\")[0].strip()\n",
    "            colab_url = f\"https://colab.research.google.com/drive/{drive_id}\"\n",
    "            file_path_to_url[file_path] = colab_url\n",
    "        else:\n",
    "            print(f\"Skipped uploading {file_path}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "for file_path in file_path_to_url.keys():\n",
    "    if file_path == \"problems.json\":\n",
    "        continue\n",
    "    problem = file_path_to_problem[file_path]\n",
    "    problem[\"metadata\"][\"colab_url\"] = file_path_to_url[file_path]\n",
    "    problem[\"metadata\"][\"file_path\"] = file_path\n",
    "    problem[\"metadata\"][\"batch_idx\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = list(file_path_to_problem.values())\n",
    "\n",
    "with open('notebooks/test/problems.json', 'w') as f:\n",
    "    json.dump(problems, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.oauth2 import service_account\n",
    "# from googleapiclient.discovery import build\n",
    "\n",
    "# # Path to your service account key file\n",
    "# SERVICE_ACCOUNT_FILE = 'creds/google__sa.json'\n",
    "\n",
    "# # The scopes required for the Sheets API\n",
    "# SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# # The ID of your spreadsheet\n",
    "# SPREADSHEET_ID = '1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4'\n",
    "\n",
    "\n",
    "# # Authenticate and build the service\n",
    "# creds = service_account.Credentials.from_service_account_file(\n",
    "#         SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "# service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "# # Specify the range and values to update\n",
    "# range_ = 'Conversations_Batch_3!A2:E610'  # For example, this updates cells from A1 to D5 in Sheet1\n",
    "# values = []\n",
    "\n",
    "# for problem in problems:\n",
    "#     values.append([\n",
    "#         problem[\"metadata\"][\"colab_url\"],\n",
    "#         problem[\"metadata\"][\"topic\"],\n",
    "#         problem[\"metadata\"][\"target_length\"],\n",
    "#         problem[\"metadata\"][\"type\"],\n",
    "#         problem[\"metadata\"][\"batch_idx\"],\n",
    "#     ])\n",
    "\n",
    "\n",
    "# body = {\n",
    "#     'values': values\n",
    "# }\n",
    "\n",
    "# # Call the Sheets API to update the range\n",
    "# request = service.spreadsheets().values().update(spreadsheetId=SPREADSHEET_ID, range=range_, valueInputOption='RAW', body=body)\n",
    "# response = request.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
